NAME: Omar Ahmad
EMAIL: omarahmad4@ucla.edu
ID: 804801272

Description of files:
lab2_list.c -- source file for program that does operations on a linked list with or without different types of synchronizations and yielding (to cause race conditions) and splits lists into lists sublists - outputs results in csv format
SortedList.h -- header file for linked list structure
SortedList.c -- implementation of linked list with forced yielding in specific locaitons if opt_yield is specified to cause race conditions
lab2_list.csv -- containing results of all test data for linked list program
graph.gp -- script to produce graphs from csv file
*.png -- graphs produced by both .gp files that show various correlations between iterations per thread, number of threads, yielding, synchronization methods, and cost per operation
README -- (this file) contains descriptions of files and answers to questions from the assignment spec
Makefile -- targets:
	 - default: builds executables lab2_add and lab2_list  with flags -Wall and -Wextra
	 - dist: creates tarball as requested by professor in autogradable format containing all files discussed above
	 - clean: removes executables, tarball, and any .o or .txt files 
	 - tests: test cases for lab2_list piped into the .csv file
	 - graphs: calls graph.gp script to produce graphs from .csv file

Sources:
man pages for 

- man pages for all functions used
       - examples found in man pages used as templates
- used previous cs35l and cs111 projects of mine to get basics
- used hash function under dijb2 from: http://www.cse.yorku.ca/~oz/hash.html


** could not make the locks actually work - resulting in incomplete data for the graphs for any amount of threads > 1 :(
** realized very late that the "random" char generator produced the same char every time - also had detrimental effects on project



Answers to questions:
QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
      - in the list operations and with few threads checking for locks is very inexpensive
      
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
      - most of the time is spent on obtaining/waiting for locks
      - if the iterations are high, then it is possible the list operations start to gain ground on the lock operations


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?
    - the while(spinLock) line consumes most of the cycles
    - this is because with large numbers of threads, most threads start to simply wait on each other rather than do actual work 


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
    - average lock wait time rises with number of threads because as threads increase, obtaining a lock becomes more and more difficult and less likely so more time is spent obtaining the lock
    - context switches remain at a consistent cost and completion time is independent of number of threads
    - wait time per op is linearly dependent on number of threads - while completion time per op is less dependent on the number of threads


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

   ** my program never really worked so answering based on what i think i should have seen **
   - as the number of sublists increases, so does the number of individual locks and so increasing threads no longer has as much a penalty as if there was only one lock because multiple threads can now do work at the same time
   - at a certain point, increasing the number of lists will have no noticable impact on the probability of contention, and after this point, the throuput will no longer increase as more lists would simply increase total overhead and slow the program down
   - in an optimal program with a good random char generator and hash function, this should be the case 